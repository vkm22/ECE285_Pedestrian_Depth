{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../PSMNet-modified/\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from stackhourglass import PSMNet\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "\n",
    "__imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "def scale_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n",
    "    t_list = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**normalize),\n",
    "    ]\n",
    "    return transforms.Compose(t_list)\n",
    "\n",
    "def get_transform(name='imagenet', input_size=None,\n",
    "                  scale_size=None, normalize=None):\n",
    "    normalize = __imagenet_stats\n",
    "    input_size = 256\n",
    "    return scale_crop(input_size=input_size, scale_size=scale_size, normalize=normalize)\n",
    "\n",
    "def test(imgL,imgR):\n",
    "    model = PSMNet(192)\n",
    "    model = nn.DataParallel(model, device_ids=[0])\n",
    "    model.cuda()\n",
    "    state_dict = torch.load('pretrained_model_KITTI2012.tar')\n",
    "    model.load_state_dict(state_dict['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    imgL = imgL[155:155+384,:,:]\n",
    "    imgR = imgR[155:155+384,:,:]\n",
    "    imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "    imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    processed = get_transform()\n",
    "\n",
    "    imgL = processed(imgL).numpy()\n",
    "    imgR = processed(imgR).numpy()\n",
    "\n",
    "    imgL = np.reshape(imgL,[1,3,imgL.shape[1],imgL.shape[2]])\n",
    "    imgR = np.reshape(imgR,[1,3,imgR.shape[1],imgR.shape[2]])\n",
    "    print(imgL.shape)\n",
    "\n",
    "    # pad to (384, 1248)\n",
    "    top_pad = 384-imgL.shape[2]\n",
    "    left_pad = 1248-imgL.shape[3]\n",
    "    imgL = np.lib.pad(imgL,((0,0),(0,0),(top_pad,0),(0,left_pad)),mode='constant',constant_values=0)\n",
    "    imgR = np.lib.pad(imgR,((0,0),(0,0),(top_pad,0),(0,left_pad)),mode='constant',constant_values=0)\n",
    "\n",
    "    imgL = torch.FloatTensor(imgL).cuda()\n",
    "    imgR = torch.FloatTensor(imgR).cuda()\n",
    "\n",
    "    imgL, imgR = Variable(imgL), Variable(imgR)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(imgL,imgR)\n",
    "    output = torch.squeeze(output)\n",
    "    pred_disp = output.data.cpu().numpy()\n",
    "\n",
    "    return pred_disp\n",
    "\n",
    "#load images\n",
    "\n",
    "pred_disp = test(image_l,image_r)\n",
    "left_pad  = 1248-946\n",
    "img = pred_disp[0:,:-left_pad]\n",
    "img = img*(255/np.max(img))\n",
    "outname = outname_prefix + str(count) + '.png'\n",
    "cv2.imshow(outname,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VID_DIR = '../videos/'\n",
    "left = cv2.VideoCapture(\"carL.mp4\")\n",
    "right = cv2.VideoCapture(\"carR.mp4\")\n",
    "outname_prefix = 'depth_cnn'\n",
    "limit = 100\n",
    "count = 0\n",
    "while count <= limit:\n",
    "    # Read in the left and right images\n",
    "    success_l, image_l = left.read()\n",
    "    success_r, image_r = right.read()\n",
    "    if count == 99:\n",
    "        left_pad  = 1248-946\n",
    "        img = pred_disp[0:,:-left_pad]\n",
    "        img = img*(255/np.max(img))\n",
    "        outname = outname_prefix + str(count) + '.png'\n",
    "        cv2.imwrite(outname,img)\n",
    "    count += 1\n",
    "    if count%20 == 0:\n",
    "        print([str(count) + '/' + str(limit)])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "outname_prefix = 'depth/depth_cnn'\n",
    "limit = 99\n",
    "video = cv2.VideoWriter('depth_video.mp4',cv2.VideoWriter_fourcc(*'mp4v'),24,(946,384),False)\n",
    "for j in range(limit):\n",
    "    img = cv2.imread(outname_prefix + str(j) + '.png')\n",
    "    video.write(img)\n",
    "video.release()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth_map_creator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH_DIR = '../input_images/depth_maps/'\n",
    "depth_img = [] \n",
    "depth_names = []\n",
    "ord_names = []\n",
    "\n",
    "print (\"Reading in computed depth maps\")\n",
    "for filename in os.listdir(DEPTH_DIR):\n",
    "    try:\n",
    "        img = skimage.io.imread(os.path.join(DEPTH_DIR, filename))\n",
    "        if img is not None:\n",
    "            depth_img.append(img)\n",
    "            depth_names.append(filename)\n",
    "            ord_names.append(int(filename[9:].split('.')[0]))\n",
    "    except:\n",
    "        print('Cant import ' + filename)\n",
    "    \n",
    "# order the images by name to know in order which frame they belong\n",
    "zipped = sorted(zip(ord_names, depth_names, depth_img))\n",
    "ord_names, depth_names, depth_img = zip(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_list(lbbox,rbbox,depth):\n",
    "    \"\"\"\n",
    "    Given depth image, list of left and right bbox coordinates and indexes,\n",
    "    find depth per object and return according to index per image\n",
    "    \"\"\"\n",
    "    out = np.zeros((20,3))\n",
    "    k = 0\n",
    "    # For each of the left bounding boxes,\n",
    "    # find the one in rbbox that is the closest in\n",
    "    # length/width, and centroids. \n",
    "    for i in range(0,lbbox.shape[0]):\n",
    "        for j in range(0,rbbox.shape[0]):\n",
    "            l_obj = lbbox[i]\n",
    "            r_obj = rbbox[j]\n",
    "            \n",
    "            print (\"Comparing \", l_obj, \" to \", r_obj)\n",
    "\n",
    "            l_x1 = l_obj[1]\n",
    "            l_x2 = l_obj[3]\n",
    "            l_y1 = l_obj[0]\n",
    "            l_y2 = l_obj[2]\n",
    "            \n",
    "            r_x1 = r_obj[1]\n",
    "            r_x2 = r_obj[3]\n",
    "            r_y1 = r_obj[0]\n",
    "            r_y2 = r_obj[2]\n",
    "\n",
    "            l_length = abs(l_x2 - l_x1)\n",
    "            l_width = abs(l_y2 - l_y1)\n",
    "            l_centroid_x = l_x1 + 0.5*l_length \n",
    "            l_centroid_y = l_y1 + 0.5*l_width\n",
    "\n",
    "            r_length = abs(r_x2 - r_x1)\n",
    "            r_width = abs(r_y2 - r_y1)\n",
    "            r_centroid_x = r_x1 + 0.5*r_length \n",
    "            r_centroid_y = r_y1 + 0.5*r_width\n",
    "\n",
    "            thresh = 20\n",
    "            print (\"Lengths: \", l_length, r_length)\n",
    "            print (\"Widths: \", l_width, r_width)\n",
    "            print (\"Centroid (x): \", l_centroid_x, r_centroid_x)\n",
    "            print (\"Centroid (y): \", l_centroid_y, r_centroid_y) \n",
    "            if (abs(l_length - r_length) < thresh and abs(l_width - r_width) < thresh):\n",
    "                depth_x1 = int((l_x1 + r_x1)/2.0)\n",
    "                depth_x2 = int((l_x2 + r_x2)/2.0)\n",
    "                depth_y1 = int((l_y1 + r_y1)/2.0)\n",
    "                depth_y2 = int((l_y2 + r_y2)/2.0)\n",
    "                depth_l_index = i\n",
    "                depth_r_index = j\n",
    "                depth_obj_matrix = depth[depth_x1:depth_x2,depth_y1:depth_y2]\n",
    "                depth_obj = np.mean(depth_obj_matrix[depth_obj_matrix > 0])\n",
    "                out[k] = [depth_obj, depth_l_index, depth_r_index]\n",
    "                k += 1\n",
    "                break \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100,101):\n",
    "    lrois = results[0][i][0]['rois'] \n",
    "    rrois = results[1][i][0]['rois']\n",
    "    left_classes = results[0][i][0]['class_ids']\n",
    "    right_classes = results[1][i][0]['class_ids']\n",
    "    \n",
    "    lbbox = []\n",
    "    rbbox = []\n",
    "    for j in range(len(lrois)): \n",
    "        if (left_classes[j] == 1): lbbox.append(lrois[j])\n",
    "    for j in range(len(rrois)): \n",
    "        if (right_classes[j] == 1): rbbox.append(rrois[j])\n",
    "            \n",
    "    lbbox = np.array(lbbox)\n",
    "    rbbox = np.array(rbbox)\n",
    "            \n",
    "    print (names[0][i], names[1][i], depth_names[i])\n",
    "    out = depth_list(lbbox, rbbox, depth_img[i])\n",
    "    print (out)\n",
    "    \n",
    "    for k in range(out.shape[0]):\n",
    "        # get the indices of each thing, and try to visualize it\n",
    "        display_imgs(\"foo\", images[0][i], lrois, r['masks'], r['class_ids'], class_names, r['scores'], show=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
