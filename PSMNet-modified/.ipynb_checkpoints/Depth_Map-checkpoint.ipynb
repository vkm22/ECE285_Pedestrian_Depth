{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"../PSMNet-modified/\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from stackhourglass import PSMNet\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "\n",
    "__imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n",
    "                   'std': [0.229, 0.224, 0.225]}\n",
    "\n",
    "def scale_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n",
    "    t_list = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**normalize),\n",
    "    ]\n",
    "    return transforms.Compose(t_list)\n",
    "\n",
    "def get_transform(name='imagenet', input_size=None,\n",
    "                  scale_size=None, normalize=None):\n",
    "    normalize = __imagenet_stats\n",
    "    input_size = 256\n",
    "    return scale_crop(input_size=input_size, scale_size=scale_size, normalize=normalize)\n",
    "\n",
    "def test(imgL,imgR):\n",
    "    model = PSMNet(192)\n",
    "    model = nn.DataParallel(model, device_ids=[0])\n",
    "    model.cuda()\n",
    "    state_dict = torch.load('pretrained_model_KITTI2012.tar')\n",
    "    model.load_state_dict(state_dict['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    imgL = imgL[155:155+384,:,:]\n",
    "    imgR = imgR[155:155+384,:,:]\n",
    "    imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "    imgR = cv2.cvtColor(imgR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    processed = get_transform()\n",
    "\n",
    "    imgL = processed(imgL).numpy()\n",
    "    imgR = processed(imgR).numpy()\n",
    "\n",
    "    imgL = np.reshape(imgL,[1,3,imgL.shape[1],imgL.shape[2]])\n",
    "    imgR = np.reshape(imgR,[1,3,imgR.shape[1],imgR.shape[2]])\n",
    "    print(imgL.shape)\n",
    "\n",
    "    # pad to (384, 1248)\n",
    "    top_pad = 384-imgL.shape[2]\n",
    "    left_pad = 1248-imgL.shape[3]\n",
    "    imgL = np.lib.pad(imgL,((0,0),(0,0),(top_pad,0),(0,left_pad)),mode='constant',constant_values=0)\n",
    "    imgR = np.lib.pad(imgR,((0,0),(0,0),(top_pad,0),(0,left_pad)),mode='constant',constant_values=0)\n",
    "\n",
    "    imgL = torch.FloatTensor(imgL).cuda()\n",
    "    imgR = torch.FloatTensor(imgR).cuda()\n",
    "\n",
    "    imgL, imgR = Variable(imgL), Variable(imgR)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(imgL,imgR)\n",
    "    output = torch.squeeze(output)\n",
    "    pred_disp = output.data.cpu().numpy()\n",
    "\n",
    "    return pred_disp\n",
    "\n",
    "#load images\n",
    "\n",
    "pred_disp = test(image_l,image_r)\n",
    "left_pad  = 1248-946\n",
    "img = pred_disp[0:,:-left_pad]\n",
    "img = img*(255/np.max(img))\n",
    "outname = outname_prefix + str(count) + '.png'\n",
    "cv2.imshow(outname,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VID_DIR = '../videos/'\n",
    "left = cv2.VideoCapture(\"carL.mp4\")\n",
    "right = cv2.VideoCapture(\"carR.mp4\")\n",
    "outname_prefix = 'depth_cnn'\n",
    "limit = 100\n",
    "count = 0\n",
    "while count <= limit:\n",
    "    # Read in the left and right images\n",
    "    success_l, image_l = left.read()\n",
    "    success_r, image_r = right.read()\n",
    "    if count == 99:\n",
    "        left_pad  = 1248-946\n",
    "        img = pred_disp[0:,:-left_pad]\n",
    "        img = img*(255/np.max(img))\n",
    "        outname = outname_prefix + str(count) + '.png'\n",
    "        cv2.imwrite(outname,img)\n",
    "    count += 1\n",
    "    if count%20 == 0:\n",
    "        print([str(count) + '/' + str(limit)])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "outname_prefix = 'depth/depth_cnn'\n",
    "limit = 99\n",
    "video = cv2.VideoWriter('depth_video.mp4',cv2.VideoWriter_fourcc(*'mp4v'),24,(946,384),False)\n",
    "for j in range(limit):\n",
    "    img = cv2.imread(outname_prefix + str(j) + '.png')\n",
    "    video.write(img)\n",
    "video.release()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
